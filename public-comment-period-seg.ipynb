{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7475a584-99a1-4ae2-bfca-f152e39a09ca",
   "metadata": {},
   "source": [
    "# Public Comment Period Segmentation via GPT-3.5-Turbo-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b594c8-4821-404c-81bf-841350af0e75",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36f2b86-84b4-4000-8c21-94f9a4e27f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>true_masses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The May 11th, 2022 meeting of the Seattle City...</td>\n",
       "      <td>{'event_id': 'b5e3673a68ff', 'session_id': 'c9...</td>\n",
       "      <td>[3133, 3043, 59419]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are recording. Wonderful. Okay. Good aftern...</td>\n",
       "      <td>{'event_id': '84bfb428c005', 'session_id': 'c4...</td>\n",
       "      <td>[1046, 563, 41062]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you. Have a great day. Good morning, eve...</td>\n",
       "      <td>{'event_id': 'c511fea02999', 'session_id': '93...</td>\n",
       "      <td>[447, 3749, 72862]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "11  The May 11th, 2022 meeting of the Seattle City...   \n",
       "0   We are recording. Wonderful. Okay. Good aftern...   \n",
       "4   Thank you. Have a great day. Good morning, eve...   \n",
       "\n",
       "                                                 meta          true_masses  \n",
       "11  {'event_id': 'b5e3673a68ff', 'session_id': 'c9...  [3133, 3043, 59419]  \n",
       "0   {'event_id': '84bfb428c005', 'session_id': 'c4...   [1046, 563, 41062]  \n",
       "4   {'event_id': 'c511fea02999', 'session_id': '93...   [447, 3749, 72862]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load annotated dataset\n",
    "df = pd.read_json(\"trial-datasets/seattle-public-comment-period-seg-v0.jsonl\", lines=True)\n",
    "\n",
    "# Prep dataset for eval\n",
    "prepped_eval_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    # Get text\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Get meta\n",
    "    meta = row[\"meta\"]\n",
    "\n",
    "    # Construct masses\n",
    "    masses = []\n",
    "    prev_index = 0\n",
    "    if isinstance(row[\"spans\"], list):\n",
    "        for span in row[\"spans\"]:\n",
    "            # Choose what index to get based off label\n",
    "            if span[\"label\"] == \"FIRST-SENTENCE\":\n",
    "                # Get start index\n",
    "                mass_calc_index = span[\"start\"]\n",
    "            else:\n",
    "                # Get end index\n",
    "                mass_calc_index = span[\"end\"]\n",
    "\n",
    "            # Add masses to list\n",
    "            masses.append(mass_calc_index - prev_index)\n",
    "\n",
    "            # Update prev index\n",
    "            prev_index = mass_calc_index\n",
    "        \n",
    "        # Add final mass\n",
    "        masses.append(len(text) - prev_index)\n",
    "    else:\n",
    "        # Add mass for full text\n",
    "        masses.append(len(text))\n",
    "\n",
    "    # Add to list\n",
    "    prepped_eval_rows.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"meta\": row[\"meta\"],\n",
    "        \"true_masses\": masses,\n",
    "    })\n",
    "\n",
    "# Convert to dataframe\n",
    "prepped_eval_df = pd.DataFrame(prepped_eval_rows)\n",
    "prepped_eval_df = prepped_eval_df.sample(3)\n",
    "prepped_eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991adf3-645e-463c-927a-1fd2aa3c2be1",
   "metadata": {},
   "source": [
    "## Prompt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcfc286-7c37-4f15-87a6-dbef487af1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import backoff\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "import spacy\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatAnthropic(model=\"claude-2.0\", temperature=0, max_tokens_to_sample=4096)\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "class PublicCommentPeriod(BaseModel):\n",
    "    first_sentence_text: str | None = Field(\n",
    "        description=\"the text of the sentence which introduces the public comment period, or null if no public comment period was found\",\n",
    "    )\n",
    "    last_sentence_text: str | None = Field(\n",
    "        description=\"the text of the sentence which concludes the public comment period, or if null no public comment period was found\",\n",
    "    )\n",
    "\n",
    "class MultiPublicCommentPeriod(BaseModel):\n",
    "    periods: list[PublicCommentPeriod] = Field(\n",
    "        description=\"the list of public comment periods (sometimes also called public hearings)\",\n",
    "    )\n",
    "\n",
    "PUBLIC_COMMENT_PERIOD_SEG_PARSER = PydanticOutputParser(pydantic_object=MultiPublicCommentPeriod)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "PUBLIC_COMMENT_PERIOD_SEG_PROMPT = PromptTemplate.from_file(\n",
    "    \"prompts/v0-public-comment-period-seg.jinja\",\n",
    "    input_variables=[\"transcript\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": PUBLIC_COMMENT_PERIOD_SEG_PARSER.get_format_instructions(),\n",
    "    },\n",
    "    template_format=\"jinja2\",\n",
    ")\n",
    "\n",
    "@backoff.on_exception(backoff.expo, json.JSONDecodeError, max_tries=3)\n",
    "def _process_transcript(text: str) -> list[int]:\n",
    "    # Convert text to sentences\n",
    "    sentences = list(nlp(text).sents)\n",
    "\n",
    "    # Convert to prompt ready string\n",
    "    transcript_str = \"\\n\\n\".join([sent.text for sent in sentences])\n",
    "\n",
    "    # Fill the prompt\n",
    "    input_ = PUBLIC_COMMENT_PERIOD_SEG_PROMPT.format_prompt(transcript=transcript_str)\n",
    "\n",
    "    # Generate\n",
    "    output = llm([HumanMessage(content=input_.to_string())])\n",
    "\n",
    "    # Parse output\n",
    "    try:\n",
    "        pc_periods = PUBLIC_COMMENT_PERIOD_SEG_PARSER.parse(output.content)\n",
    "\n",
    "    except:\n",
    "        print(output.content)\n",
    "        raise Exception(\"Failed to parse output\")\n",
    "\n",
    "    # Process all periods found\n",
    "    prev_index = 0\n",
    "    predicted_masses = []\n",
    "    for pc_period in pc_periods.periods:\n",
    "        # Process masses\n",
    "        if (\n",
    "            pc_period.first_sentence_text is not None\n",
    "            and pc_period.last_sentence_text is not None\n",
    "        ):\n",
    "            first_sentence_index = text.find(pc_period.first_sentence_text)\n",
    "            predicted_masses.append(first_sentence_index - prev_index)\n",
    "            prev_index = first_sentence_index\n",
    "\n",
    "            last_sentence_index = text.find(pc_period.last_sentence_text)\n",
    "            predicted_masses.append(last_sentence_index - prev_index)\n",
    "            prev_index = last_sentence_index\n",
    "\n",
    "    # Add final mass (or full text as mass)\n",
    "    if len(predicted_masses) == 0:\n",
    "        predicted_masses.append(len(text))\n",
    "    else:\n",
    "        predicted_masses.append(len(text) - prev_index)\n",
    "\n",
    "    return predicted_masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728245c-f246-4b37-990c-85f5fdf313de",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625e7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import segeval\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for _, row in tqdm(prepped_eval_df.iterrows(), total=len(prepped_eval_df)):\n",
    "    # Get masses\n",
    "    predicted_masses = _process_transcript(row[\"text\"])\n",
    "\n",
    "    # Get similarity\n",
    "    sim = segeval.boundary_similarity(row[\"true_masses\"], predicted_masses, n_t=int(len(row[\"text\"]) * 0.07))\n",
    "\n",
    "    # Get confusion matrix\n",
    "    matrix = segeval.boundary_confusion_matrix(row[\"true_masses\"], predicted_masses, n_t=int(len(row[\"text\"]) * 0.07))\n",
    "\n",
    "    # Get precision, recall, and f1\n",
    "    precision = segeval.precision(matrix)\n",
    "    recall = segeval.recall(matrix)\n",
    "    f1 = segeval.fmeasure(matrix)\n",
    "\n",
    "    # Add to results\n",
    "    results.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"meta\": row[\"meta\"],\n",
    "        \"true_masses\": row[\"true_masses\"],\n",
    "        \"predicted_masses\": predicted_masses,\n",
    "        \"similarity\": sim,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    })\n",
    "\n",
    "# Convert to dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Mean Similarity:\", results_df[\"similarity\"].mean())\n",
    "print(\"Mean Precision:\", results_df[\"precision\"].mean())\n",
    "print(\"Mean Recall:\", results_df[\"recall\"].mean())\n",
    "print(\"Mean F1:\", results_df[\"f1\"].mean())\n",
    "print()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Similarity:\", results_df[\"similarity\"].mean())\n",
    "print(\"Mean Precision:\", results_df[\"precision\"].mean())\n",
    "print(\"Mean Recall:\", results_df[\"recall\"].mean())\n",
    "print(\"Mean F1:\", results_df[\"f1\"].mean())\n",
    "print()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368900e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
