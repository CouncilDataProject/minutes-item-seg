{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7475a584-99a1-4ae2-bfca-f152e39a09ca",
   "metadata": {},
   "source": [
    "# Public Comment Period Segmentation via GPT-3.5-Turbo-Instruct\n",
    "\n",
    "## High Level Overview\n",
    "\n",
    "This notebook demonstrates how to use Large Language Models to segment out the \"public comment\" and \"public hearing\" portions of municipal meeting transcripts. There may be multiple of these periods within a single meeting.\n",
    "\n",
    "The way that this is setup is we prompt an LLM to record the \"first_sentence_text\" and the \"last_sentence_text\" for each period. In addition we specify how the model should output the processed result (a structured JSON object). You can view the whole prompt in the [./prompts/v0-public-comment-period-seg.jinja](./prompts/v0-public-comment-period-seg.jinja) file.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "I manually annotated 12 meeting transcripts from the Seattle CDP dataset, marking their first and last sentence of the public comment periods. If I recall correctly, there were 3 meetings that had multiple public comment periods (or public hearings).\n",
    "\n",
    "We convert the annotated dataset into a DataFrame with the full text of the transcript, the metadata of the event, and the \"masses\" of the segmentation, the masses tell the system how long each portion of the meeting is. For example, if the masses are: `[100, 200, 100]` the portions might be something like a string 100 characters long that is unclassified, then a string of 200 characters long that we assume is the annotated public comment period, and finally another string 100 characters long to close out the meeting.\n",
    "\n",
    "In the case of multiple comment periods, the masses might look like: `[100, 200, 100, 200, 100]` (not-comment, comment, not-comment, comment, not-comment).\n",
    "\n",
    "## LLMs\n",
    "\n",
    "I have tested with a few LLMs: OpenAI's gpt-3.5-turbo, gpt-3.5-turbo-instruct-16k, gpt-3.5-turbo-16k, and gpt-4; Anthropic's claude-2-100k; and [an open-source fine-tuned version of Meta's llama-2](https://huggingface.co/Yukang/Llama-2-7b-longlora-100k-ft).\n",
    "\n",
    "The latest commit of this notebook uses Anthropic's Claude 2 for demonstration because it is the simpliest model to use. The others do not have context windows large enough to fit the whole transcript into context and need to be partitioned. There are methods for handling that that showed promising results but for the sake of demonstration, I will stick with Claude-2.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "We use `segeval` to evaluate the performance of the system. In general we want to compare the relative distances between reference public comment segmentation boundaries and the predicted public comment segmentation boundaries. The closer they are the better, however because of how much administrative \"cruft\" is in the transcript / meeting, there does need to be some flexibility here because the clerk or the chair of the meeting may talk about opening the public comment period multiple times in the adminstration portion of transitioning to public comment. For example, they might say \"Let's move on to public comment\" and then after describing how public comment will work (time allotment, rules, etc.) they will finally say \"We are now open to public comment, our first commenter is Eva.\" It isn't clear which one is \"better\" for segmentation and the system may choose either so we have to allow for tolerance but otherwise, `segeval` works great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b594c8-4821-404c-81bf-841350af0e75",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36f2b86-84b4-4000-8c21-94f9a4e27f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>true_masses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you. Have a great day. Good morning, eve...</td>\n",
       "      <td>{'event_id': 'c511fea02999', 'session_id': '93...</td>\n",
       "      <td>[447, 3749, 72862]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The May 11th, 2022 meeting of the Seattle City...</td>\n",
       "      <td>{'event_id': 'b5e3673a68ff', 'session_id': 'c9...</td>\n",
       "      <td>[3133, 3043, 59419]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Good morning, everybody. The March 3, 2021 mee...</td>\n",
       "      <td>{'event_id': '749503d88894', 'session_id': 'f6...</td>\n",
       "      <td>[1671, 10938, 1491, 1725, 144754]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "4   Thank you. Have a great day. Good morning, eve...   \n",
       "11  The May 11th, 2022 meeting of the Seattle City...   \n",
       "10  Good morning, everybody. The March 3, 2021 mee...   \n",
       "\n",
       "                                                 meta  \\\n",
       "4   {'event_id': 'c511fea02999', 'session_id': '93...   \n",
       "11  {'event_id': 'b5e3673a68ff', 'session_id': 'c9...   \n",
       "10  {'event_id': '749503d88894', 'session_id': 'f6...   \n",
       "\n",
       "                          true_masses  \n",
       "4                  [447, 3749, 72862]  \n",
       "11                [3133, 3043, 59419]  \n",
       "10  [1671, 10938, 1491, 1725, 144754]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load annotated dataset\n",
    "df = pd.read_json(\"trial-datasets/seattle-public-comment-period-seg-v0.jsonl\", lines=True)\n",
    "\n",
    "# Prep dataset for eval\n",
    "prepped_eval_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    # Get text\n",
    "    text = row[\"text\"]\n",
    "    \n",
    "    # Get meta\n",
    "    meta = row[\"meta\"]\n",
    "\n",
    "    # Construct masses\n",
    "    masses = []\n",
    "    prev_index = 0\n",
    "    if isinstance(row[\"spans\"], list):\n",
    "        for span in row[\"spans\"]:\n",
    "            # Choose what index to get based off label\n",
    "            if span[\"label\"] == \"FIRST-SENTENCE\":\n",
    "                # Get start index\n",
    "                mass_calc_index = span[\"start\"]\n",
    "            else:\n",
    "                # Get end index\n",
    "                mass_calc_index = span[\"end\"]\n",
    "\n",
    "            # Add masses to list\n",
    "            masses.append(mass_calc_index - prev_index)\n",
    "\n",
    "            # Update prev index\n",
    "            prev_index = mass_calc_index\n",
    "        \n",
    "        # Add final mass\n",
    "        masses.append(len(text) - prev_index)\n",
    "    else:\n",
    "        # Add mass for full text\n",
    "        masses.append(len(text))\n",
    "\n",
    "    # Add to list\n",
    "    prepped_eval_rows.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"meta\": row[\"meta\"],\n",
    "        \"true_masses\": masses,\n",
    "    })\n",
    "\n",
    "# Convert to dataframe\n",
    "prepped_eval_df = pd.DataFrame(prepped_eval_rows)\n",
    "prepped_eval_df = prepped_eval_df.sample(3)\n",
    "prepped_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991adf3-645e-463c-927a-1fd2aa3c2be1",
   "metadata": {},
   "source": [
    "## Prompt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e792af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eva/miniforge-pypy3/envs/minutes-item-seg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import backoff\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "import spacy\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatAnthropic(model=\"claude-2.0\", temperature=0, max_tokens_to_sample=4096)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=4096)\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     endpoint_url=\"https://boxjj56zj0zbbjue.us-east-1.aws.endpoints.huggingface.cloud\",\n",
    "#     task=\"text2text-generation\",\n",
    "# )\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "class PublicCommentPeriod(BaseModel):\n",
    "    first_sentence_text: str | None = Field(\n",
    "        description=\"the text of the sentence which introduces the public comment period, or null if no public comment period was found\",\n",
    "    )\n",
    "    last_sentence_text: str | None = Field(\n",
    "        description=\"the text of the sentence which concludes the public comment period, or if null no public comment period was found\",\n",
    "    )\n",
    "\n",
    "class MultiPublicCommentPeriod(BaseModel):\n",
    "    periods: list[PublicCommentPeriod] = Field(\n",
    "        description=\"the list of public comment periods (sometimes also called public hearings)\",\n",
    "    )\n",
    "\n",
    "PUBLIC_COMMENT_PERIOD_SEG_PARSER = PydanticOutputParser(pydantic_object=MultiPublicCommentPeriod)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "PUBLIC_COMMENT_PERIOD_SEG_PROMPT = PromptTemplate.from_file(\n",
    "    \"prompts/v0-public-comment-period-seg.jinja\",\n",
    "    input_variables=[\"transcript\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": PUBLIC_COMMENT_PERIOD_SEG_PARSER.get_format_instructions(),\n",
    "    },\n",
    "    template_format=\"jinja2\",\n",
    ")\n",
    "\n",
    "@backoff.on_exception(backoff.expo, json.JSONDecodeError, max_tries=3)\n",
    "def _process_transcript(text: str) -> list[int]:\n",
    "    # Convert text to sentences\n",
    "    sentences = list(nlp(text).sents)\n",
    "\n",
    "    # Convert to prompt ready string\n",
    "    transcript_str = \"\\n\\n\".join([sent.text for sent in sentences[:300]])\n",
    "\n",
    "    # Fill the prompt\n",
    "    input_ = PUBLIC_COMMENT_PERIOD_SEG_PROMPT.format_prompt(transcript=transcript_str)\n",
    "\n",
    "    # Generate\n",
    "    if isinstance(llm, BaseChatModel):\n",
    "        # Generate\n",
    "        output = llm([HumanMessage(content=input_.to_string())]).content\n",
    "    else:\n",
    "        # Generate\n",
    "        output = llm(input_.to_string())\n",
    "\n",
    "    # Parse output\n",
    "    try:\n",
    "        pc_periods = PUBLIC_COMMENT_PERIOD_SEG_PARSER.parse(output)\n",
    "\n",
    "    except:\n",
    "        print(output)\n",
    "        raise Exception(\"Failed to parse output\")\n",
    "\n",
    "    # Process all periods found\n",
    "    prev_index = 0\n",
    "    predicted_masses = []\n",
    "    for pc_period in pc_periods.periods:\n",
    "        # Process masses\n",
    "        if (\n",
    "            pc_period.first_sentence_text is not None\n",
    "            and pc_period.last_sentence_text is not None\n",
    "        ):\n",
    "            first_sentence_index = text.find(pc_period.first_sentence_text)\n",
    "            predicted_masses.append(first_sentence_index - prev_index)\n",
    "            prev_index = first_sentence_index\n",
    "\n",
    "            last_sentence_index = text.find(pc_period.last_sentence_text)\n",
    "            predicted_masses.append(last_sentence_index - prev_index)\n",
    "            prev_index = last_sentence_index\n",
    "\n",
    "    # Add final mass (or full text as mass)\n",
    "    if len(predicted_masses) == 0:\n",
    "        predicted_masses.append(len(text))\n",
    "    else:\n",
    "        predicted_masses.append(len(text) - prev_index)\n",
    "\n",
    "    return predicted_masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728245c-f246-4b37-990c-85f5fdf313de",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625e7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [33:50<00:00, 676.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Similarity: 0.8734748287940417\n",
      "Mean Precision: 1.0\n",
      "Mean Recall: 1.0\n",
      "Mean F1: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>meta</th>\n",
       "      <th>true_masses</th>\n",
       "      <th>predicted_masses</th>\n",
       "      <th>similarity</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you. Have a great day. Good morning, eve...</td>\n",
       "      <td>{'event_id': 'c511fea02999', 'session_id': '93...</td>\n",
       "      <td>[447, 3749, 72862]</td>\n",
       "      <td>[447, 3583, 73028]</td>\n",
       "      <td>0.978452751817237798546209761</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The May 11th, 2022 meeting of the Seattle City...</td>\n",
       "      <td>{'event_id': 'b5e3673a68ff', 'session_id': 'c9...</td>\n",
       "      <td>[3133, 3043, 59419]</td>\n",
       "      <td>[982, 5101, 59512]</td>\n",
       "      <td>0.657822506861848124428179323</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good morning, everybody. The March 3, 2021 mee...</td>\n",
       "      <td>{'event_id': '749503d88894', 'session_id': 'f6...</td>\n",
       "      <td>[1671, 10938, 1491, 1725, 144754]</td>\n",
       "      <td>[1671, 10878, 1218, 1942, 144870]</td>\n",
       "      <td>0.9841492277030393622321873442</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Thank you. Have a great day. Good morning, eve...   \n",
       "1  The May 11th, 2022 meeting of the Seattle City...   \n",
       "2  Good morning, everybody. The March 3, 2021 mee...   \n",
       "\n",
       "                                                meta  \\\n",
       "0  {'event_id': 'c511fea02999', 'session_id': '93...   \n",
       "1  {'event_id': 'b5e3673a68ff', 'session_id': 'c9...   \n",
       "2  {'event_id': '749503d88894', 'session_id': 'f6...   \n",
       "\n",
       "                         true_masses                   predicted_masses  \\\n",
       "0                 [447, 3749, 72862]                 [447, 3583, 73028]   \n",
       "1                [3133, 3043, 59419]                 [982, 5101, 59512]   \n",
       "2  [1671, 10938, 1491, 1725, 144754]  [1671, 10878, 1218, 1942, 144870]   \n",
       "\n",
       "                       similarity precision recall f1  \n",
       "0   0.978452751817237798546209761         1      1  1  \n",
       "1   0.657822506861848124428179323         1      1  1  \n",
       "2  0.9841492277030393622321873442         1      1  1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import segeval\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for _, row in tqdm(prepped_eval_df.iterrows(), total=len(prepped_eval_df)):\n",
    "    # Get masses\n",
    "    predicted_masses = _process_transcript(row[\"text\"])\n",
    "\n",
    "    # Get similarity\n",
    "    #\n",
    "    # The parameter `n_t` allows for greater \"translational\" edits \n",
    "    # rather than \"addition\" or \"deletion\" edits.\n",
    "    # how this is actually computed is looking at the \n",
    "    # relative distances between \"boundary marks\"\n",
    "    # i.e.\n",
    "    # a reference segementation looks like:    0000000100000001000000010001000\n",
    "    # and a predicted segmentation looks like: 0000010000000000010000001001000\n",
    "    # however depending on how far away the boundary is,\n",
    "    # it can be considered an addition, deletion, or translation\n",
    "    # additions and deletions are penelized more heavily than translations\n",
    "    # in our case, there is a lot of fuzziness around \"what is the \n",
    "    # true first start sentence and last end sentence because there is lots of admin\n",
    "    # so to allow for this fuzziness, we boost the \"n (allowed) T(ranslation)\"\n",
    "    #\n",
    "    # Ultimately what this says is that we allow translation to be ~0.5% of the\n",
    "    # transcript different\n",
    "    sim = segeval.boundary_similarity(row[\"true_masses\"], predicted_masses, n_t=int(len(row[\"text\"]) * 0.05))\n",
    "\n",
    "    # Get confusion matrix\n",
    "    matrix = segeval.boundary_confusion_matrix(row[\"true_masses\"], predicted_masses, n_t=int(len(row[\"text\"]) * 0.05))\n",
    "\n",
    "    # Get precision, recall, and f1\n",
    "    precision = segeval.precision(matrix)\n",
    "    recall = segeval.recall(matrix)\n",
    "    f1 = segeval.fmeasure(matrix)\n",
    "\n",
    "    # Add to results\n",
    "    results.append({\n",
    "        \"text\": row[\"text\"],\n",
    "        \"meta\": row[\"meta\"],\n",
    "        \"true_masses\": row[\"true_masses\"],\n",
    "        \"predicted_masses\": predicted_masses,\n",
    "        \"similarity\": sim,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    })\n",
    "\n",
    "# Convert to dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Mean Similarity:\", results_df[\"similarity\"].mean())\n",
    "print(\"Mean Precision:\", results_df[\"precision\"].mean())\n",
    "print(\"Mean Recall:\", results_df[\"recall\"].mean())\n",
    "print(\"Mean F1:\", results_df[\"f1\"].mean())\n",
    "print()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4885cf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_masses</th>\n",
       "      <th>predicted_masses</th>\n",
       "      <th>similarity</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[447, 3749, 72862]</td>\n",
       "      <td>[447, 3583, 73028]</td>\n",
       "      <td>0.978452751817237798546209761</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3133, 3043, 59419]</td>\n",
       "      <td>[982, 5101, 59512]</td>\n",
       "      <td>0.657822506861848124428179323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1671, 10938, 1491, 1725, 144754]</td>\n",
       "      <td>[1671, 10878, 1218, 1942, 144870]</td>\n",
       "      <td>0.9841492277030393622321873442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         true_masses                   predicted_masses  \\\n",
       "0                 [447, 3749, 72862]                 [447, 3583, 73028]   \n",
       "1                [3133, 3043, 59419]                 [982, 5101, 59512]   \n",
       "2  [1671, 10938, 1491, 1725, 144754]  [1671, 10878, 1218, 1942, 144870]   \n",
       "\n",
       "                       similarity f1  \n",
       "0   0.978452751817237798546209761  1  \n",
       "1   0.657822506861848124428179323  1  \n",
       "2  0.9841492277030393622321873442  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[[\"true_masses\", \"predicted_masses\", \"similarity\", \"f1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09944c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
